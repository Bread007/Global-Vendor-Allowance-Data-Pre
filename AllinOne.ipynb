{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allowance(Coops and Deals) Data Cleasing\n",
    "\n",
    "##### Data Sources: DB2 and SAP\n",
    "##### Markets: \n",
    "-               US\n",
    "-               CA\n",
    "-               CN\n",
    "-               IN (No deals in India market)\n",
    "-               AR (Deal amount data for AR are not accessible)\n",
    "\n",
    "### 01. Fill Deal Amount Data to Master Data (DB2)\n",
    "1.1 In SAP Data, columns names are different\n",
    "\n",
    "1.2 Import DB2 Data\n",
    "\n",
    "1.3 Fill Deal amount data from SAP to the DB2 master table\n",
    "\n",
    "1.4 Validate SAP amount records\n",
    "\n",
    "### 02. Process Exchange Rate Data\n",
    "\n",
    "### 03. Derived additional datafields (columns) based on raw data and reference data\n",
    "3.1 Process the 32-38 datafields \n",
    "\n",
    "3.2 Process the 39-45 datafields \n",
    "\n",
    "3.3 Process the 46-53 datafields \n",
    "\n",
    "3.4 Process the 53-55 datafields \n",
    "\n",
    "3.5 Process the 56-57 datafields (Risk Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Fill Deal Amount Data to Master Data (DB2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.1 In SAP Data, columns names are different\n",
    "-  1) Deal_ID is named as \"Reference\"\n",
    "-  2) Amount is named 'Amount in doc. curr.'(CN),'Amount in local currency'(US,CA)\n",
    "-  3) 1 Deal_ID have multiple rows, because of different posting dates\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set up diretory, where the SAP data is stored\n",
    "\n",
    "os.chdir(r'\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Coop_Deal_Raw_Data\\SAP_Deal_Amt')\n",
    "\n",
    "\n",
    "\n",
    "# Import SAP Data\n",
    "# because the format or the volume of the data, it takes time to read in below 3 files\n",
    "df_01 = pd.ExcelFile('VA_CN_SAP.xlsx').parse('Sheet1')\n",
    "df_02 = pd.ExcelFile('VA_CAN_SAP.xlsx').parse('Sheet1')\n",
    "df_03 = pd.ExcelFile('VA_US_SAP_A110.xlsx').parse('Sheet1')\n",
    "\n",
    "\n",
    "\n",
    "## opening the origin SAP file will make the excel disfunction\n",
    "## maybe because the format or the volume of the data\n",
    "## in below I write the SAP data to new formats to make the file openable, so that validation could be conducted easily\n",
    "\n",
    "# for the last row of the origin data is total amount, remove that row\n",
    "df_01[0:-1].to_excel('VA_CN_Deal.xlsx', sheet_name='sheet1', index=False)\n",
    "df_02[0:-1].to_excel('VA_CAN_Deal.xlsx', sheet_name='sheet1', index=False)\n",
    "df_03[0:-1].to_excel('VA_US_Deal.xlsx', sheet_name='sheet1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Creat function to summarize total amount for each allowance ID\n",
    "def deal_sum(df,amt_col_name,country_name):\n",
    "    df_deal_amt = pd.pivot_table(df,\n",
    "                                 index = ['Reference'],\n",
    "                                 values = [amt_col_name], # e.g. 'Amount in doc. curr.'\n",
    "                                 aggfunc={amt_col_name:np.sum})\n",
    "    # convert the indices to column\n",
    "    df_deal_amt = df_deal_amt.reset_index() \n",
    "    \n",
    "    # rename the columns of the data frame\n",
    "    df_deal_amt.columns = ['DEAL_ID','AMOUNT_SAP']\n",
    "    \n",
    "    # creat a new column to identify the contry name, for 3 SAP summarized data needed to be concate later\n",
    "    df_deal_amt['COUNTRY'] = country_name\n",
    "    \n",
    "    # Change the data type of DEAL_ID\n",
    "    df_deal_amt['DEAL_ID'] = df_deal_amt['DEAL_ID'].apply(lambda x: int(x))\n",
    "    return (df_deal_amt)\n",
    "\n",
    "# summarize total amount for each allowance ID for each country\n",
    "df_deal_CN  = deal_sum (df_01[0:-1],'Amount in doc. curr.','CN')\n",
    "df_deal_CA  = deal_sum (df_02[0:-1],'Amount in local currency','CA')\n",
    "df_deal_US  = deal_sum (df_03[0:-1],'Amount in local currency','US')\n",
    "\n",
    "# concate deal amount data to 1 file\n",
    "df_deal_all = pd.concat([df_deal_CN,df_deal_CA,df_deal_US],ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.2 Import DB2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set up diretory, where the DB2 Data is stored\n",
    "\n",
    "os.chdir(r'\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Coop_Deal_Raw_Data\\DB2')\n",
    "\n",
    "# import 5 files in such folder all at once\n",
    "# because the volume of the data, it takes time to read in all files in the folder\n",
    "\n",
    "files = os.listdir(r'\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Coop_Deal_Raw_Data\\DB2')\n",
    "\n",
    "frames = {f[:2]:pd.ExcelFile(f).parse('Sheet1')\n",
    "          for f in files if f.endswith('.xlsx')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concate 5 files to 1 file, so that no need to join and merge multiple times\n",
    "db2_CoopDeal = pd.concat([frames[i] for i in list(frames.keys())],ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.3 Fill Deal amount data from SAP to the DB2 master table\n",
    "- 1 deal may have mulitple row of records  because the amount could be allocated to mulitple departments\n",
    "- In other words, DEAL_IDs are the same, but 'VENDR_DEPT_NBR's aren't\n",
    "\n",
    "- When filling deal amount to DB2 master table, if the deal id have mulitiple rows,  the amount will be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_db2_SAP = pd.merge(db2_CoopDeal,df_deal_all,how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ix =  df_db2_SAP[(df_db2_SAP['DEAL_OR_COOP'] =='D')&(df_db2_SAP['AMOUNT_SAP'].notnull())].index\n",
    "\n",
    "df_db2_SAP.loc[ix,'AMOUNT'] = df_db2_SAP.loc[ix,'AMOUNT_SAP'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Write the consolidated allowance data \n",
    "### So that no need to rerun the code from the every beginning\n",
    "os.chdir(r'\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Coop_Deal_Raw_Data\\DB2')\n",
    "\n",
    "df_db2_SAP.to_csv('Coop_Deal_DB2_SAP.csv', index = False,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  1.4 Validate SAP amount records\n",
    "- if any deal id in DB2 is not in SAP\n",
    "- if any deal id in SAP is not in DB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Unique Deal ID in DB2', 84968)\n",
      "('Unique Deal ID in SAP', 72922)\n",
      "19609 Deals exist in SAP but NOT in DB2\n",
      "31655 Deals exist in DB2 but NOT in SAP\n"
     ]
    }
   ],
   "source": [
    "# Check unique Deal IDs in SAP and DB2 Data, to see if any deal id in DB2 is not in SAP\n",
    "\n",
    "db2_Deal_IDs = np.unique(db2_CoopDeal[db2_CoopDeal['DEAL_OR_COOP'] =='D']['DEAL_ID'])\n",
    "SAP_Deal_IDs = np.unique(df_deal_all['DEAL_ID'])\n",
    "\n",
    "print('Unique Deal ID in DB2',len(db2_Deal_IDs ))\n",
    "print('Unique Deal ID in SAP',len(SAP_Deal_IDs))\n",
    "\n",
    "\n",
    "# Deals In DB2 not in SAP\n",
    "Deal_No_SAP = list(set(db2_Deal_IDs) - set(SAP_Deal_IDs))\n",
    "Deal_No_DB2 = list(set(SAP_Deal_IDs) - set(db2_Deal_IDs))\n",
    "\n",
    "print(' '.join([str(len(Deal_No_DB2)),'Deals exist in SAP but NOT in DB2']))\n",
    "print(' '.join([str(len(Deal_No_SAP)),'Deals exist in DB2 but NOT in SAP']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>Tot_Deal_Count</th>\n",
       "      <th>Deal_NoAmt_Count</th>\n",
       "      <th>NoAmt_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR</td>\n",
       "      <td>10498</td>\n",
       "      <td>10498</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA</td>\n",
       "      <td>35775</td>\n",
       "      <td>3289</td>\n",
       "      <td>9.193571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CN</td>\n",
       "      <td>36059</td>\n",
       "      <td>16719</td>\n",
       "      <td>46.365678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>3075</td>\n",
       "      <td>1447</td>\n",
       "      <td>47.056911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  COUNTRY  Tot_Deal_Count  Deal_NoAmt_Count     NoAmt_%\n",
       "0      AR           10498             10498  100.000000\n",
       "1      CA           35775              3289    9.193571\n",
       "2      CN           36059             16719   46.365678\n",
       "3      US            3075              1447   47.056911"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_No_Deal_amt =df_db2_SAP[(df_db2_SAP['DEAL_OR_COOP'] =='D')&(df_db2_SAP['AMOUNT_SAP'].isnull())]\n",
    "\n",
    "No_Deal_amt_Country = pd.pivot_table(df_No_Deal_amt,\n",
    "                                 index = ['COUNTRY'],\n",
    "                                 values = ['DEAL_ID'], # e.g. 'Amount in doc. curr.'\n",
    "                                 aggfunc={'DEAL_ID':'count'})\n",
    "No_Deal_amt_Country = No_Deal_amt_Country.reset_index()\n",
    "\n",
    "\n",
    "df_Deal_all = df_db2_SAP[(df_db2_SAP['DEAL_OR_COOP'] =='D')]\n",
    "Deal_Country = pd.pivot_table(df_Deal_all,\n",
    "                                 index = ['COUNTRY'],\n",
    "                                 values = ['DEAL_ID'], # e.g. 'Amount in doc. curr.'\n",
    "                                 aggfunc={'DEAL_ID':'count'})\n",
    "Deal_Country = Deal_Country.reset_index()\n",
    "\n",
    "Deal_NoAmt_Sum = pd.merge(Deal_Country,No_Deal_amt_Country,how = 'left', on ='COUNTRY')\n",
    "\n",
    "Deal_NoAmt_Sum.rename(columns={'DEAL_ID_x': 'Tot_Deal_Count',\n",
    "                               'DEAL_ID_y': 'Deal_NoAmt_Count'}, inplace=True)\n",
    "\n",
    "Deal_NoAmt_Sum['NoAmt_%'] = Deal_NoAmt_Sum['Deal_NoAmt_Count']/Deal_NoAmt_Sum['Tot_Deal_Count']*100\n",
    "\n",
    "Deal_NoAmt_Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. Process Exchange Rate Data\n",
    "- Average monthly exchange rates are used to convert local currency to US dollar\n",
    "- Average monthly exchange rates are provided by Finance team\n",
    "- Link for files of average monthly exchange rates: https://walmartglobal.box.com/s/ahnbopxzvgy3kr1fv5uvqs6mem5l779t\n",
    "- files in such link were downloaded to this folder: \n",
    "\n",
    "'\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Foriegn Exchange rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set up diretory, where the files of average monthly exchange rates are stored\n",
    "\n",
    "os.chdir(r'\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Foriegn Exchange rates')\n",
    "\n",
    "\n",
    "# import 25 files in such folder all at once\n",
    "# because the volume of the data, it takes time to read in all files in the folder\n",
    "\n",
    "files = os.listdir(r'\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Foriegn Exchange rates')\n",
    "\n",
    "df_exrate = [f for f in files if f.endswith('.xlsx')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concate all the 25 files\n",
    "\n",
    "curr_list =['ARS','CAD','CLP','CNY','CRC','GBP' ,'GTQ' ,'HNL' ,'INR' ,'JPY','MXN','NIO','SVC','USD']\n",
    "country_list = ['AR','CA','CL','CN','CR','GB','GT','HN','IN','JP','MX','NI','SV']\n",
    "\n",
    "df_ex_GVA = pd.DataFrame()\n",
    "for i in range(len(df_exrate)):\n",
    "    df_ex = pd.ExcelFile(df_exrate[i]).parse('Y - avg rate')\n",
    "    ix =df_ex.iloc[4:,0][df_ex.iloc[4:,0].apply(lambda x:x.rstrip()).apply(lambda x: x in curr_list )].index\n",
    "    df_ex_new = df_ex.iloc[ix,[0,1]]\n",
    "    df_ex_new['COUNTRY'] = country_list\n",
    "    df_ex_new['Rate_Time'] = df_ex.iloc[3,1]\n",
    "    df_ex_new.columns = ['Currency','Ex_Rate','COUNTRY','Time']\n",
    "    \n",
    "    df_ex_GVA = pd.concat([df_ex_GVA,df_ex_new],ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the consolidate Exchange Rate file to this folder: \"Reference Tables\", for later reference\n",
    "# \\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Reference Tables\n",
    "\n",
    "os.chdir(r'\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Reference Tables')\n",
    "df_ex_GVA.to_csv('Exchange_Rate_GVA_2018.csv', index = False,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. Derived additional datafields (columns) based on raw data and reference data\n",
    "- for derived additional datafields explanation, refer to this file \"Data Resources.xlsx\"\n",
    "\n",
    "\n",
    "- in this folder :\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Query_DB2_PO\n",
    "\n",
    "\n",
    "- datafields are derived based on the sequence in Data Resources.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  3.1 Process the 32-38 datafields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 31 - Year\n",
    "## 32 - Month\n",
    "\n",
    "df_db2_SAP['Year'] = df_db2_SAP['DEAL_STATUS_DATE'].apply(lambda x: x.year)\n",
    "df_db2_SAP['Month'] = df_db2_SAP['DEAL_STATUS_DATE'].apply(lambda x: x.month)\n",
    "df_ex_GVA['Month'] = df_ex_GVA['Time'].apply(lambda x: x.month)\n",
    "df_ex_GVA['Year'] = df_ex_GVA['Time'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 33 - 'First_Yr'\n",
    "\n",
    "# Yr_1: 08/01/2016 – 07/31/2017\n",
    "# Yr_2:08/01/2017 – 07/31/2018\n",
    "    \n",
    "df_db2_SAP['First_Yr'] = 'Yr_2'\n",
    "\n",
    "df_db2_SAP.loc[df_db2_SAP[df_db2_SAP['Year']<=2016].index,'First_Yr'] ='Yr_1'\n",
    "    \n",
    "ix_1 = df_db2_SAP[(df_db2_SAP['Year'] ==2017) & (df_db2_SAP['Month'] <=7)].index\n",
    "df_db2_SAP.loc[ix_1,'First_Yr'] ='Yr_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 35 Ex_Rate\n",
    "## 36 Time\n",
    "## 37 Currency\n",
    "\n",
    "# set a different name for the new dataframe in case the merge result is incorrect\n",
    "df_db2_SAP_1 = pd.merge(df_db2_SAP,df_ex_GVA,\n",
    "                        how = 'left',on=['COUNTRY','Year','Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 38 AMT_US_Dollar\n",
    "df_db2_SAP_1['AMT_US_Dollar'] =df_db2_SAP_1['AMOUNT']/df_db2_SAP_1['Ex_Rate']\n",
    "\n",
    "####### Attention: for some reason, the 'Ex_Rate' is not available \n",
    "####### 1) if the allowance is for US market\n",
    "####### 2) the months deal_status_date is not included in the Exchange Rate Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  3.2 Process the 39-45 datafields \n",
    "\n",
    "- in this section, multiple referece data/look up tables needed to be imported\n",
    "\n",
    "\n",
    "1) VA_Group_GVA_2018_11_6 DC.xlsx\n",
    "\n",
    "\n",
    "2) Status_Group_GVA.xlsx\n",
    "\n",
    "\n",
    "3) Dept_Desc_WM1_WW_CORE_DIM_VM.xlsx (This table contains department descriptions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  3.2.1 39 - 40 Allowance Group & Volume/Rebates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 39 Allowance Group\n",
    "# 40 Volume/Rebates\n",
    "\n",
    "# Import the reference table created by GVA team : VA_Group_GVA_2018_11_6 DC\n",
    "\n",
    "os.chdir(r'\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Reference Tables')\n",
    "df_ref_1 =  pd.ExcelFile('VA_Group_GVA_2018_11_6 DC.xlsx').parse('Sheet3')\n",
    "\n",
    "# set a different name for the new dataframe in case the merge result is incorrect\n",
    "df_db2_SAP_2 = pd.merge(df_db2_SAP_1,\n",
    "                        df_ref_1[['ALLOWANCE_TYPE_Ori','Allowance Group','Volume/Rebates']],\n",
    "                        how = 'left',\n",
    "                        left_on='ALLOWANCE_TYPE', right_on='ALLOWANCE_TYPE_Ori')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below ALLOWANCE_TYPE are not categorized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>ALLOWANCE_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28992</th>\n",
       "      <td>CA</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77939</th>\n",
       "      <td>CN</td>\n",
       "      <td>Inbound transportation services fee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78478</th>\n",
       "      <td>CN</td>\n",
       "      <td>Vendor Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78534</th>\n",
       "      <td>CN</td>\n",
       "      <td>Dishonesty Loss Compensation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79467</th>\n",
       "      <td>CN</td>\n",
       "      <td>Warehouse Packsell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79505</th>\n",
       "      <td>CN</td>\n",
       "      <td>Government Inspection relating Fee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81378</th>\n",
       "      <td>CN</td>\n",
       "      <td>Return Services Fee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82436</th>\n",
       "      <td>CN</td>\n",
       "      <td>Violate food safety operation rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83554</th>\n",
       "      <td>CN</td>\n",
       "      <td>Cost Reduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392688</th>\n",
       "      <td>CN</td>\n",
       "      <td>Membership Benefit Brochure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470759</th>\n",
       "      <td>US</td>\n",
       "      <td>MDSE Bus Trips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499440</th>\n",
       "      <td>US</td>\n",
       "      <td>Prepaid Wireless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503860</th>\n",
       "      <td>IN</td>\n",
       "      <td>18 - NEW STORE / ANNIVERSARY DISCOUNTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       COUNTRY                          ALLOWANCE_TYPE\n",
       "28992       CA                                    None\n",
       "77939       CN     Inbound transportation services fee\n",
       "78478       CN                          Vendor Support\n",
       "78534       CN            Dishonesty Loss Compensation\n",
       "79467       CN                      Warehouse Packsell\n",
       "79505       CN      Government Inspection relating Fee\n",
       "81378       CN                     Return Services Fee\n",
       "82436       CN     Violate food safety operation rules\n",
       "83554       CN                          Cost Reduction\n",
       "392688      CN             Membership Benefit Brochure\n",
       "470759      US                          MDSE Bus Trips\n",
       "499440      US                        Prepaid Wireless\n",
       "503860      IN  18 - NEW STORE / ANNIVERSARY DISCOUNTS"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the datapoints in 'Allowance Group'\n",
    "\n",
    "print(\"Below ALLOWANCE_TYPE are not categorized\")\n",
    "df_db2_SAP_2[df_db2_SAP_2['Allowance Group'].isnull()][['COUNTRY','ALLOWANCE_TYPE']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>ALLOWANCE_TYPE</th>\n",
       "      <th>Volume/Rebates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28992</th>\n",
       "      <td>CA</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77939</th>\n",
       "      <td>CN</td>\n",
       "      <td>Inbound transportation services fee</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78478</th>\n",
       "      <td>CN</td>\n",
       "      <td>Vendor Support</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78534</th>\n",
       "      <td>CN</td>\n",
       "      <td>Dishonesty Loss Compensation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79467</th>\n",
       "      <td>CN</td>\n",
       "      <td>Warehouse Packsell</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79505</th>\n",
       "      <td>CN</td>\n",
       "      <td>Government Inspection relating Fee</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81378</th>\n",
       "      <td>CN</td>\n",
       "      <td>Return Services Fee</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82436</th>\n",
       "      <td>CN</td>\n",
       "      <td>Violate food safety operation rules</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83554</th>\n",
       "      <td>CN</td>\n",
       "      <td>Cost Reduction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392688</th>\n",
       "      <td>CN</td>\n",
       "      <td>Membership Benefit Brochure</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470759</th>\n",
       "      <td>US</td>\n",
       "      <td>MDSE Bus Trips</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499440</th>\n",
       "      <td>US</td>\n",
       "      <td>Prepaid Wireless</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503860</th>\n",
       "      <td>IN</td>\n",
       "      <td>18 - NEW STORE / ANNIVERSARY DISCOUNTS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       COUNTRY                          ALLOWANCE_TYPE Volume/Rebates\n",
       "28992       CA                                    None            NaN\n",
       "77939       CN     Inbound transportation services fee            NaN\n",
       "78478       CN                          Vendor Support            NaN\n",
       "78534       CN            Dishonesty Loss Compensation            NaN\n",
       "79467       CN                      Warehouse Packsell            NaN\n",
       "79505       CN      Government Inspection relating Fee            NaN\n",
       "81378       CN                     Return Services Fee            NaN\n",
       "82436       CN     Violate food safety operation rules            NaN\n",
       "83554       CN                          Cost Reduction            NaN\n",
       "392688      CN             Membership Benefit Brochure            NaN\n",
       "470759      US                          MDSE Bus Trips            NaN\n",
       "499440      US                        Prepaid Wireless            NaN\n",
       "503860      IN  18 - NEW STORE / ANNIVERSARY DISCOUNTS            NaN"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the datapoints in 'Volume/Rebates'\n",
    "df_db2_SAP_2[df_db2_SAP_2['Volume/Rebates'].isnull()][['COUNTRY','ALLOWANCE_TYPE','Volume/Rebates']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  3.2.2  41 DEAL_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ref_2 =  pd.ExcelFile('Status_Group_GVA.xlsx').parse('sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for COOP, there are multple data types for 'DEAL_STATUS_CODE' \n",
      "[<type 'unicode'> <type 'long'>]\n"
     ]
    }
   ],
   "source": [
    "ix1  = df_db2_SAP_2[df_db2_SAP_2['DEAL_OR_COOP'] =='C'].index\n",
    "\n",
    "print(\"for COOP, there are multple data types for 'DEAL_STATUS_CODE' \")\n",
    "print(df_db2_SAP_2.loc[ix1,'DEAL_STATUS_CODE'].apply(lambda x: type(x)).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<type 'unicode'>]\n"
     ]
    }
   ],
   "source": [
    "ix2  = df_db2_SAP_2[df_db2_SAP_2['DEAL_OR_COOP'] =='D'].index\n",
    "\n",
    "print(df_db2_SAP_2.loc[ix2,'DEAL_STATUS_CODE'].apply(lambda x: type(x)).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since the 'DEAL_STATUS_CODE' extacted from DB2 are of different data types\n",
    "# convert 'DEAL_STATUS_CODE' to the same data type --  String\n",
    "\n",
    "\n",
    "\n",
    "ix  = df_db2_SAP_2[df_db2_SAP_2['DEAL_OR_COOP'] =='C'].index\n",
    "df_db2_SAP_2.loc[ix,'DEAL_STATUS_CODE'] = df_db2_SAP_2.loc[ix,'DEAL_STATUS_CODE'].apply(lambda x: str(int(x)).strip())\n",
    "\n",
    "ix = set(df_db2_SAP_2.index) - set(ix)\n",
    "df_db2_SAP_2.loc[ix,'DEAL_STATUS_CODE'] = df_db2_SAP_2.loc[ix,'DEAL_STATUS_CODE'].apply(lambda x: x.strip())\n",
    "df_ref_2['DEAL_STATUS_CODE']= df_ref_2['DEAL_STATUS_CODE'].apply(lambda x: str(x))\n",
    "\n",
    "\n",
    "df_db2_SAP_3 = pd.merge(df_db2_SAP_2,df_ref_2,\n",
    "                    how = 'left',\n",
    "                    left_on = [u'DEAL_OR_COOP',u'DEAL_STATUS_CODE'],\n",
    "                     right_on = [u'DEAL_OR_COOP',u'DEAL_STATUS_CODE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>DEAL_ID</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>DEAL_OR_COOP</th>\n",
       "      <th>DEAL_STATUS_DATE</th>\n",
       "      <th>ALLOWANCE_TYPE</th>\n",
       "      <th>BASE_DIV_NBR</th>\n",
       "      <th>ACCT_DIV_NBR</th>\n",
       "      <th>SAMS_CATG_NBR</th>\n",
       "      <th>BUYER_NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>First_Yr</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Ex_Rate</th>\n",
       "      <th>Time</th>\n",
       "      <th>AMT_US_Dollar</th>\n",
       "      <th>ALLOWANCE_TYPE_Ori</th>\n",
       "      <th>Allowance Group</th>\n",
       "      <th>Volume/Rebates</th>\n",
       "      <th>DEAL_STATUS_DESC_y</th>\n",
       "      <th>DEAL_STATUS_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [COUNTRY, DEAL_ID, AMOUNT, DEAL_OR_COOP, DEAL_STATUS_DATE, ALLOWANCE_TYPE, BASE_DIV_NBR, ACCT_DIV_NBR, SAMS_CATG_NBR, BUYER_NAME, EVENT_BEGIN_DATE, EVENT_END_DATE, VENDR_NBR, VENDR_DEPT_NBR, VENDR_NAME, DEAL_STATUS_CODE, DEAL_STATUS_DESC_x, DEAL_STATUS_x, 3_MONTH_INDICATOR, CHARGE_FMT, CHARGE_BASIS, DEPT_COUNT_SAMS, DEPT_COUNT_WMT, VERSION_COUNT, ALLOWANCE_ID, BUYER_ID, VENDOR_ID, DMM_ID, AR_ID, BUYER_APPROVAL_DATE, VENDOR_APPROVAL_DATE, DMM_APPROVAL_DATE, AR_APPROVAL_DATE, AMOUNT_SAP, Year, Month, First_Yr, Currency, Ex_Rate, Time, AMT_US_Dollar, ALLOWANCE_TYPE_Ori, Allowance Group, Volume/Rebates, DEAL_STATUS_DESC_y, DEAL_STATUS_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 46 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the deal status group \n",
    "df_db2_SAP_3[df_db2_SAP_3['DEAL_STATUS_y'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean up the column names after merging\n",
    "df_db2_SAP_3 = df_db2_SAP_3.drop([u'ALLOWANCE_ID', 'ALLOWANCE_TYPE_Ori','DEAL_STATUS_DESC_y','DEAL_STATUS_x'],axis=1)\n",
    "\n",
    "df_db2_SAP_3.rename(columns={u'DEAL_STATUS_DESC_x':'DEAL_STATUS_DESC',\n",
    "                             'DEAL_STATUS_y': 'DEAL_STATUS'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  3.2.3 42-45  Business_Format/DEPT_DESC/MDSE_SUBGROUP_DESC/MDSE_SEGMENT_DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### 42 Business Format\n",
    "\n",
    "df_db2_SAP_3.loc[:,'Business_Format'] ='WMT'\n",
    "\n",
    "#SAMS\n",
    "df_db2_SAP_3.loc[df_db2_SAP_3[df_db2_SAP_3['BASE_DIV_NBR']==18].index,'Business_Format'] = 'Sams'\n",
    "\n",
    "#WMT.COM\n",
    "bool1 = df_db2_SAP_3['ACCT_DIV_NBR']==46\n",
    "df_db2_SAP_3.loc[df_db2_SAP_3[bool1].index,'Business_Format'] ='WMT.COM'\n",
    "\n",
    "### Notice: \n",
    "# In india market, only sams exist in this time period 2016-08-01 to 2018-07-31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### 43 - 45\n",
    "\n",
    "# Import department description file\n",
    "\n",
    "df_ref_3 = pd.ExcelFile('Dept_Desc_WM1_WW_CORE_DIM_VM.xlsx').parse('Sheet1')\n",
    "\n",
    "# Column name for department number                        -- DEPT_NBR\n",
    "# Column name for department description                   -- DEPT_DESC\n",
    "# Column name for business units(walmart or sam's)         -- BASE_DIV_NBR\n",
    "# Column name for country names                            -- COUNTRY_CODE\n",
    "# Column name for MDSE subgroup                            -- MDSE_SUBGROUP_DESC\n",
    "# Column name for MDSE segment                             -- MDSE_SEGMENT_DESC\n",
    "\n",
    "#### MDSE segment > MDSE subgroup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dept = df_ref_3[df_ref_3['CURRENT_IND'] =='Y'][['COUNTRY_CODE','BASE_DIV_NBR','DEPT_NBR','DEPT_DESC','MDSE_SUBGROUP_DESC','MDSE_SEGMENT_DESC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_db2_SAP_4 = pd.merge(df_db2_SAP_3,df_dept,how = 'left',\n",
    "                        left_on = ['COUNTRY','BASE_DIV_NBR', 'VENDR_DEPT_NBR'],\n",
    "                        right_on = ['COUNTRY_CODE','BASE_DIV_NBR', 'DEPT_NBR'])\n",
    "\n",
    "# clean up the column names after merging\n",
    "df_db2_SAP_4 = df_db2_SAP_4.drop(['COUNTRY_CODE','DEPT_NBR', ],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  3.3 Process the 46-53 datafields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Notice, in DB2 data format, \n",
    "##### some records of EVENT_END_DATE and EVENT_BEGIN_DATE is '?'\n",
    "\n",
    "##### to clean data from other systems or from markets, be careful about the data type of the date\n",
    "\n",
    "col = ['EVENT_BEGIN_DATE','EVENT_END_DATE','AR_APPROVAL_DATE',\n",
    "       #'BUYER_APPROVAL_DATE', --  No Null value under this date\n",
    "       'DMM_APPROVAL_DATE']\n",
    "\n",
    "# to fill the null value or \"?\" in any date columns with \"Unknown\"\n",
    "\n",
    "##### EVENT_BEGIN_DATE:      datetime.datetime\n",
    "##### EVENT_END_DATE:        datetime.datetime\n",
    "##### AR_APPROVAL_DATE:      datetime.datetime\n",
    "##### BUYER_APPROVAL_DATE:   'pandas.tslib.Timestamp'\n",
    "##### DMM_APPROVAL_DATE:     datetime.datetime\n",
    "\n",
    "for i in col:\n",
    "    \n",
    "    df_db2_SAP_4.loc[:,i] = df_db2_SAP_4[i].fillna('Unknown') \n",
    "    \n",
    "    ix = df_db2_SAP_4[df_db2_SAP_4[i]=='?'].index\n",
    "    df_db2_SAP_4.loc[ix,i] ='Unknown'\n",
    "\n",
    "#######################################################\n",
    "## Function to calculate month gap \n",
    "#######################################################\n",
    "def month_gap (date1,date2):\n",
    "    if (date1=='Unknown')or (date2=='Unknown'):\n",
    "        month_gap = 'Unknown'\n",
    "        \n",
    "    elif str(type(date2)) == \"<class 'pandas.tslib.Timestamp'>\":\n",
    "        year_2 = date2.date().year\n",
    "        month_2 = date2.date().month\n",
    "        \n",
    "        year_1 = date1.year\n",
    "        month_1 = date1.month\n",
    "        month_gap = (year_2-year_1)*12 + (month_2-month_1)  \n",
    "        \n",
    "    else:\n",
    "        year_1 = date1.year\n",
    "        month_1 = date1.month\n",
    "        \n",
    "        year_2 = date2.year\n",
    "        month_2 = date2.month     \n",
    "        month_gap = (year_2-year_1)*12 + (month_2-month_1)       \n",
    "    return (month_gap)\n",
    "\n",
    "#######################################################\n",
    "## Function to calculate day gap \n",
    "#######################################################\n",
    "def day_gap (date1,date2):\n",
    "    if (date1=='Unknown')or (date2=='Unknown'):\n",
    "        day_gap = 'Unknown'\n",
    "    else: \n",
    "        day_gap = int((date2 - date1).days)\n",
    "    return (day_gap)\n",
    "        \n",
    "#######################################################\n",
    "## Function to apply functions \n",
    "#######################################################\n",
    "def lag_cal(df,func,date1,date2):\n",
    "    lag = [func(df[date1][i],df[date2][i]) for i in df.index]\n",
    "    return(lag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 46 C_D_Duration : EVENT_END_DATE  - EVENT_BEGIN_DATE\n",
    "df_db2_SAP_4['C_D_Duration'] = lag_cal(df_db2_SAP_4,month_gap,'EVENT_BEGIN_DATE','EVENT_END_DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 47 Mul_Month_Coop\n",
    "df_db2_SAP_4['Mul_Month_Coop'] ='N'\n",
    "bool1 = (df_db2_SAP_4['C_D_Duration']>=2) & (df_db2_SAP_4['DEAL_OR_COOP'] == 'C')\n",
    "ix = df_db2_SAP_4[bool1].index\n",
    "df_db2_SAP_4.loc[ix,'Mul_Month_Coop'] ='Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 48 EBD_AR_Day\n",
    "# 49 EED_AR_Day\n",
    "df_db2_SAP_4['EBD_AR_Day'] = lag_cal(df_db2_SAP_4,day_gap,'EVENT_BEGIN_DATE','AR_APPROVAL_DATE')\n",
    "df_db2_SAP_4['EED_AR_Day'] = lag_cal(df_db2_SAP_4,day_gap,'EVENT_END_DATE','AR_APPROVAL_DATE') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 50 EventDay_AR_Month\n",
    "# 51 EventDay_Buyer_Month\n",
    "# 52 EventDay_DMM_Month\n",
    "\n",
    "ix_c = df_db2_SAP_4[df_db2_SAP_4['DEAL_OR_COOP'] == 'C'].index\n",
    "ix_d = df_db2_SAP_4[df_db2_SAP_4['DEAL_OR_COOP'] == 'D'].index\n",
    "\n",
    "df_db2_SAP_4['EventDay_AR_Month'] = np.nan\n",
    "df_db2_SAP_4['EventDay_Buyer_Month'] = np.nan\n",
    "df_db2_SAP_4['EventDay_DMM_Month'] = np.nan\n",
    "\n",
    "df_db2_SAP_4.loc[ix_c,'EventDay_AR_Month'] = lag_cal(df_db2_SAP_4.loc[ix_c,:],month_gap,'EVENT_END_DATE','AR_APPROVAL_DATE')\n",
    "df_db2_SAP_4.loc[ix_d,'EventDay_AR_Month'] = lag_cal(df_db2_SAP_4.loc[ix_d,:],month_gap,'EVENT_BEGIN_DATE','AR_APPROVAL_DATE')\n",
    "\n",
    "\n",
    "df_db2_SAP_4.loc[ix_c,'EventDay_Buyer_Month'] = lag_cal(df_db2_SAP_4.loc[ix_c,:],month_gap,'EVENT_END_DATE','BUYER_APPROVAL_DATE')\n",
    "df_db2_SAP_4.loc[ix_d,'EventDay_Buyer_Month'] = lag_cal(df_db2_SAP_4.loc[ix_d,:],month_gap,'EVENT_BEGIN_DATE','BUYER_APPROVAL_DATE')\n",
    "\n",
    "df_db2_SAP_4.loc[ix_c,'EventDay_DMM_Month'] = lag_cal(df_db2_SAP_4.loc[ix_c,:],month_gap,'EVENT_END_DATE','DMM_APPROVAL_DATE')\n",
    "df_db2_SAP_4.loc[ix_d,'EventDay_DMM_Month'] = lag_cal(df_db2_SAP_4.loc[ix_d,:],month_gap,'EVENT_BEGIN_DATE','DMM_APPROVAL_DATE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  3.4 Process the 53-55 datafields \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 53 Old_Policy_Time\n",
    "# Old Policy Execution Period: before Oct 31, 2017\n",
    "df_db2_SAP_4['Old_Policy_Time'] = 'N'\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[df_db2_SAP_4['Year']<=2016].index,'Old_Policy_Time'] ='Y'    \n",
    "ix_1 = df_db2_SAP_4[(df_db2_SAP_4['Year'] ==2017) & (df_db2_SAP_4['Month'] <=10)].index\n",
    "df_db2_SAP_4.loc[ix_1,'Old_Policy_Time'] ='Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 54 Compliant_or_Not\n",
    "\n",
    "df_db2_SAP_4['Compliant_or_Not'] =np.nan\n",
    "# have DMM signiture\n",
    "bool_10 = (df_db2_SAP_4['EventDay_DMM_Month'] != 'Unknown')\n",
    "bool_11 = df_db2_SAP_4['EventDay_DMM_Month']<=0\n",
    "bool_12 = (df_db2_SAP_4['EventDay_DMM_Month']>0)&(df_db2_SAP_4['EventDay_DMM_Month']<=1)\n",
    "bool_13 = df_db2_SAP_4['EventDay_DMM_Month']>=2\n",
    "\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[bool_10 & bool_11].index,'Compliant_or_Not'] = 'Compliant'\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[bool_10 & bool_12].index,'Compliant_or_Not'] = 'TBD'\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[bool_10 & bool_13].index,'Compliant_or_Not'] = 'Not Compliant'\n",
    "\n",
    "# have AR signiture, but no DMM signiture\n",
    "bool_10 = (df_db2_SAP_4['EventDay_DMM_Month'] == 'Unknown') & (df_db2_SAP_4['EventDay_AR_Month'] != 'Unknown')\n",
    "bool_11 = df_db2_SAP_4['EventDay_AR_Month']<=0\n",
    "bool_12 = (df_db2_SAP_4['EventDay_AR_Month']>0)&(df_db2_SAP_4['EventDay_AR_Month']<=1)\n",
    "bool_13 = df_db2_SAP_4['EventDay_AR_Month']>=2\n",
    "\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[bool_10 & bool_11].index,'Compliant_or_Not'] = 'Compliant'\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[bool_10 & bool_12].index,'Compliant_or_Not'] = 'TBD'\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[bool_10 & bool_13].index,'Compliant_or_Not'] = 'Not Compliant'\n",
    "\n",
    "# only have buyer signiture\n",
    "bool_10 = (df_db2_SAP_4['EventDay_DMM_Month'] == 'Unknown')&(df_db2_SAP_4['EventDay_AR_Month'] == 'Unknown')\n",
    "bool_11 =df_db2_SAP_4['EventDay_Buyer_Month']<=0\n",
    "bool_12 = (df_db2_SAP_4['EventDay_Buyer_Month']>0)&(df_db2_SAP_4['EventDay_Buyer_Month']<=1)\n",
    "bool_13 = df_db2_SAP_4['EventDay_Buyer_Month']>1\n",
    "\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[bool_10 & bool_11].index,'Compliant_or_Not'] = 'Compliant'\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[bool_10 & bool_12].index,'Compliant_or_Not'] = 'TBD'\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[bool_10 & bool_13].index,'Compliant_or_Not'] = 'Not Compliant'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_db2_SAP_4['Compl_Old_Policy'] =np.nan\n",
    "\n",
    "bool_D_old = (df_db2_SAP_4['DEAL_OR_COOP'] == 'D') & (df_db2_SAP_4['Old_Policy_Time'] =='Y') &(df_db2_SAP_4['EBD_AR_Day'] >= 16)\n",
    "bool_C_old = (df_db2_SAP_4['DEAL_OR_COOP'] == 'C') & (df_db2_SAP_4['Old_Policy_Time'] =='Y') &(df_db2_SAP_4['EED_AR_Day']>= 31)\n",
    "\n",
    "\n",
    "ix_D_old =  df_db2_SAP_4[bool_D_old].index\n",
    "ix_C_old =  df_db2_SAP_4[bool_C_old].index\n",
    "\n",
    "\n",
    "df_db2_SAP_4.loc[ix_D_old,'Compl_Old_Policy'] = 'Not Compliant'\n",
    "df_db2_SAP_4.loc[ix_C_old,'Compl_Old_Policy'] = 'Not Compliant'\n",
    "\n",
    "ix_Comp_old = df_db2_SAP_4[(df_db2_SAP_4['Old_Policy_Time'] =='Y') &(df_db2_SAP_4['Compl_Old_Policy'].isnull())].index\n",
    "df_db2_SAP_4.loc[ix_Comp_old,'Compl_Old_Policy'] = 'Compliant'\n",
    "\n",
    "ix_Comp_new = df_db2_SAP_4[df_db2_SAP_4['Compl_Old_Policy'].isnull()].index\n",
    "df_db2_SAP_4.loc[ix_Comp_new,'Compl_Old_Policy'] = df_db2_SAP_4.loc[ix_Comp_new,'Compliant_or_Not']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.5 Process the 56-57 datafields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 56 Fst2_digit\n",
    "\n",
    "def fst2_digit_extraction(x1):\n",
    "    x1 = str(x1).replace('.','') \n",
    "    digit = x1[0:2]\n",
    "    return(digit)\n",
    "\n",
    "df_db2_SAP_4['Fst2_digit']= df_db2_SAP_4['AMOUNT'].apply(lambda x:fst2_digit_extraction(abs(x)))\n",
    "\n",
    "\n",
    "# fix amount less than 1, e.g 0.016\n",
    "ix = df_db2_SAP_4[df_db2_SAP_4['Fst2_digit'].apply(lambda x: x.startswith('0'))].index\n",
    "df_db2_SAP_4.loc[ix,'Fst2_digit'] = df_db2_SAP_4.loc[ix,'AMOUNT'].apply(lambda x: str(float(x)*100)[0:2])\n",
    "\n",
    "# fix amount less than 1 and with less than 3 decimals, e.g 0.01\n",
    "ix = df_db2_SAP_4[df_db2_SAP_4['Fst2_digit'].apply(lambda x: x.endswith('.'))].index\n",
    "df_db2_SAP_4.loc[ix,'Fst2_digit'] = df_db2_SAP_4.loc[ix,'Fst2_digit'].apply(lambda x: int(x.split('.')[0])*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BFL_Prob\n",
    "bfl_Fst2_digit= []\n",
    "for i in range(1,10):\n",
    "    for j in range(10):\n",
    "        bfl_Fst2_digit = bfl_Fst2_digit + [int(str(i)+str(j))]\n",
    "bfl_Fst2_digit_dist = [4.14,3.78,3.48,3.22,3.00,2.80,2.63,2.48,2.35,2.23,\n",
    "                      2.12,2.02,1.93,1.85,1.77,1.70,1.64,1.58,1.52,1.47,\n",
    "                      1.42,1.38,1.34,1.30,1.26,1.22,1.19,1.16,1.13,1.10,\n",
    "                      1.07,1.05,1.02,1.00,0.98,0.95,0.93,0.91,0.90,0.88,\n",
    "                       0.86,0.84,0.83,0.81,0.80,0.78,0.77,0.76,0.74,0.73,\n",
    "                       0.72,0.71,0.69,0.68,0.67,0.66,0.65,0.64,0.63,0.62,\n",
    "                       0.62,0.61,0.60,0.59,0.58,0.58,0.57,0.56,0.55,0.55,\n",
    "                       0.54,0.53,0.53,0.52,0.51,0.51,0.50,0.50,0.49,0.49,\n",
    "                       0.48,0.47,0.47,0.46,0.46,0.45,0.45,0.45,0.44,0.44]\n",
    "\n",
    "digit_prob = pd.DataFrame()\n",
    "digit_prob['Fst2_digit'] = bfl_Fst2_digit \n",
    "digit_prob['BFL_Prob']  =bfl_Fst2_digit_dist\n",
    "\n",
    "df_db2_SAP_4 = pd.merge(df_db2_SAP_4,digit_prob,how = 'left',on ='Fst2_digit' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.5.1 Process the 12 Risk Indicators\n",
    "\n",
    "- R01 Tot_Appr_Levels\n",
    "- R02 Dup_Authorization\n",
    "- R03 Duplicate\n",
    "- R04 Round_Dollar\n",
    "- R05 Buyer_Lst_5_days\n",
    "- R06 Vendor_Lst_5_days\n",
    "- R07 DMM_Lst_5_days\n",
    "- R08 AR_Lst_5_days\n",
    "- R09 All_Lst_5_days\n",
    "- R10 BuyerInAR\n",
    "- R11 VendorInAR\n",
    "- R12 DMMInAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# R01 Tot_Appr_Levels\n",
    "\n",
    "ID_col = [i for i in df_db2_SAP_4.columns  if i.endswith('ID')]\n",
    "\n",
    "for i in [2,3,4]:   \n",
    "    \n",
    "    # every allowances in DB2 has Buyer ID\n",
    "    # but not necessarily have Vendor ID, DMM ID, AR ID, if the allowace has no such IDs, its value is '?'\n",
    "    # when apply this for loop to data from other system, please check the IDs\n",
    "   \n",
    "    ix  = df_db2_SAP_4[df_db2_SAP_4[ID_col[i]] == '?'].index\n",
    "    df_db2_SAP_4.loc[ix,ID_col[i]] = np.nan\n",
    "    \n",
    "    # use 'apply'function to a dataframe, use axis = 1, apply function rowwise\n",
    "df_db2_SAP_4['R01_Tot_Appr_Levels'] = 4- df_db2_SAP_4[ID_col].isnull().apply(np.sum, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# R02 Dup_Authorization\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in [1,2,3,4]:\n",
    "    for j in [2,3,4]:\n",
    "        if (i!=j)&(j>i): \n",
    "            col = '_vs_'.join([ID_col[i].split('_')[0],ID_col[j].split('_')[0]])\n",
    "            df[col] = df_db2_SAP_4[ID_col[i]] == df_db2_SAP_4[ID_col[j]]\n",
    "            \n",
    "df_db2_SAP_4['R02 Dup_Authorization'] = df.apply(np.sum, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# R03 Duplicate\n",
    "\n",
    "#same vendor,*same year, same month, same amount, *same deal status\n",
    "\n",
    "Duplicate_1 = pd.pivot_table(df_db2_SAP_4,index = ['COUNTRY','VENDR_NBR','Month','Year','AMOUNT','DEAL_STATUS'],\n",
    "                             values = ['DEAL_ID'],\n",
    "                             aggfunc={u'DEAL_ID':'count'})\n",
    "Duplicate_1 = Duplicate_1.reset_index()\n",
    "Duplicate_1.rename(columns={'DEAL_ID': 'R03 Duplicate'},inplace=True) \n",
    "Duplicate_1 = Duplicate_1[(Duplicate_1['AMOUNT']!=0)&(Duplicate_1['R03 Duplicate']!=1)]\n",
    "df_db2_SAP_4 = pd.merge(df_db2_SAP_4,Duplicate_1,how ='left', on = ['COUNTRY', 'VENDR_NBR', 'Month', 'Year','AMOUNT','DEAL_STATUS'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# R04 Round_Dollar\n",
    "# US,MX,UK,CN,SA ---  round dollar in thousand\n",
    "# AR,CL,CAM,IN,JP---  round dollar in ten thousand\n",
    "df_db2_SAP_4['R04 Round_Dollar'] =0\n",
    "country_bool_1 = [df_db2_SAP_4['COUNTRY'][i] in ['US','MX','UK','CN','SA','CA'] for i in df_db2_SAP_4.index]\n",
    "country_bool_2 = [df_db2_SAP_4['COUNTRY'][i] in ['AR','CHILE','CAM','IN','JP'] for i in df_db2_SAP_4.index]\n",
    "round_bool_1 = (df_db2_SAP_4['AMOUNT']/1000).apply(round)*1000 ==df_db2_SAP_4['AMOUNT']\n",
    "round_bool_2 = (df_db2_SAP_4['AMOUNT']/10000).apply(round)*10000 ==df_db2_SAP_4['AMOUNT']\n",
    "\n",
    "round_ix_1 = df_db2_SAP_4[(country_bool_1)&(round_bool_1)].index\n",
    "round_ix_2 = df_db2_SAP_4[(country_bool_2)&(round_bool_2)].index\n",
    "\n",
    "df_db2_SAP_4.loc[round_ix_1,'R04 Round_Dollar'] =1\n",
    "df_db2_SAP_4.loc[round_ix_2,'R04 Round_Dollar'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# R05 Buyer_Lst_5_days\n",
    "# R06 Vendor_Lst_5_days\n",
    "# R07 DMM_Lst_5_days\n",
    "# R08 AR_Lst_5_days\n",
    "# R09 All_Lst_5_days\n",
    "\n",
    "def lst_day(x):\n",
    "    if x =='?' or x =='Unknown':gap = 1000\n",
    "    else:\n",
    "        yr = x.year\n",
    "        month = x.month\n",
    "        if month ==12: lst_day = (date(yr+1, 1, 1) - date(yr, month, 1)).days\n",
    "        else: lst_day = (date(yr, month+1, 1) - date(yr, month, 1)).days\n",
    "        gap = lst_day - x.day\n",
    "    return(gap)\n",
    "\n",
    "Appr_Date_col = ['BUYER_APPROVAL_DATE','VENDOR_APPROVAL_DATE','DMM_APPROVAL_DATE','AR_APPROVAL_DATE']\n",
    "for i in range(4):\n",
    "    #print(i)\n",
    "    risk_name = ''.join(['R0',str(i+5),' ',Appr_Date_col[i].split('_')[0],'_Lst_5_days'])\n",
    "    df_db2_SAP_4[risk_name] = list(df_db2_SAP_4[Appr_Date_col[i]].apply(lst_day))\n",
    "    \n",
    "lst5day_col = [i for i in df_db2_SAP_4.columns  if 'Lst_5_days' in i]\n",
    "\n",
    "temp = df_db2_SAP_4[lst5day_col]<5\n",
    "all_appr5_bool = df_db2_SAP_4['R01_Tot_Appr_Levels'] == temp.apply(np.sum, axis=1)\n",
    "\n",
    "df_db2_SAP_4['R09 All_Lst_5_days'] = 0\n",
    "df_db2_SAP_4.loc[df_db2_SAP_4[all_appr5_bool].index,'R09 All_Lst_5_days'] =1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# R10 BuyerInAR\n",
    "# R11 VendorInAR\n",
    "# R12 DMMInAR\n",
    "\n",
    "BuyerInAR =  set(np.unique(df_db2_SAP_4['BUYER_ID'])).intersection(np.unique(df_db2_SAP_4['AR_ID']))\n",
    "\n",
    "VendorInAR = set(np.unique(df_db2_SAP_4['VENDOR_ID'].dropna())).intersection(np.unique(df_db2_SAP_4['AR_ID']))\n",
    "\n",
    "DMMInAR  = set(np.unique(df_db2_SAP_4['DMM_ID'].dropna())).intersection(np.unique(df_db2_SAP_4['AR_ID']))\n",
    "\n",
    "df_db2_SAP_4['R10 BuyerInAR'] = df_db2_SAP_4['BUYER_ID'].apply(lambda x: x in BuyerInAR)\n",
    "df_db2_SAP_4['R11 VendorInAR'] = df_db2_SAP_4['VENDOR_ID'].apply(lambda x: x in VendorInAR)\n",
    "df_db2_SAP_4['R12 DMMInAR'] = df_db2_SAP_4['DMM_ID'].apply(lambda x: x in DMMInAR )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write files for each country\n",
    "\n",
    "os.chdir(r'\\\\phont80025us.homeoffice.wal-mart.com\\Shares\\Audit_Analytics\\FY19\\Projects\\Global Vendor Allowance\\Data\\Coop_Deal_Raw_Data\\Coop_Deal_Pro_Data')\n",
    "print_ix = []\n",
    "for i in df_db2_SAP_4['COUNTRY'].unique():\n",
    "    df_db2_SAP_4[df_db2_SAP_4['COUNTRY']==i].to_csv(''.join(['Coop_Deal_w_Amt_Risk','_',i,'.csv']),index = False,encoding='utf_8_sig')\n",
    "    #print_ix = print_ix + list(df_all_dsb[df_all_dsb['COUNTRY']==i].index)\n",
    "    \n",
    "    #print(''.join(['df_dsb','_',i,'.csv']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Anaconda2]",
   "language": "python",
   "name": "Python [Anaconda2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
